{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Using PySpark and MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read a CSV into Spark\n",
    "2. Do some trivial data wrangling with dataframes\n",
    "3. Perform a linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read a CSV into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the pyspark sql functions necessary for Spark DataFrame operations \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['street,city,zip,state,beds,baths,sq__ft,type,sale_date,price,latitude,longitude',\n",
       " '3526 HIGH ST,SACRAMENTO,95838,CA,2,1,836,Residential,Wed May 21 00:00:00 EDT 2008,59222,38.631913,-121.434879',\n",
       " '51 OMAHA CT,SACRAMENTO,95823,CA,3,1,1167,Residential,Wed May 21 00:00:00 EDT 2008,68212,38.478902,-121.431028',\n",
       " '2796 BRANCH ST,SACRAMENTO,95815,CA,2,1,796,Residential,Wed May 21 00:00:00 EDT 2008,68880,38.618305,-121.443839',\n",
       " '2805 JANETTE WAY,SACRAMENTO,95815,CA,2,1,852,Residential,Wed May 21 00:00:00 EDT 2008,69307,38.616835,-121.439146']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file into RDD\n",
    "rdd = sc.textFile('/resources/data/mllibdata/Sacramentorealestatetransactions.csv')\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Do some trivial data wrangling with dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['street',\n",
       "  'city',\n",
       "  'zip',\n",
       "  'state',\n",
       "  'beds',\n",
       "  'baths',\n",
       "  'sq__ft',\n",
       "  'type',\n",
       "  'sale_date',\n",
       "  'price',\n",
       "  'latitude',\n",
       "  'longitude'],\n",
       " ['3526 HIGH ST',\n",
       "  'SACRAMENTO',\n",
       "  '95838',\n",
       "  'CA',\n",
       "  '2',\n",
       "  '1',\n",
       "  '836',\n",
       "  'Residential',\n",
       "  'Wed May 21 00:00:00 EDT 2008',\n",
       "  '59222',\n",
       "  '38.631913',\n",
       "  '-121.434879']]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split dataset on the commas\n",
    "rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['3526 HIGH ST',\n",
       "  'SACRAMENTO',\n",
       "  '95838',\n",
       "  'CA',\n",
       "  '2',\n",
       "  '1',\n",
       "  '836',\n",
       "  'Residential',\n",
       "  'Wed May 21 00:00:00 EDT 2008',\n",
       "  '59222',\n",
       "  '38.631913',\n",
       "  '-121.434879'],\n",
       " ['51 OMAHA CT',\n",
       "  'SACRAMENTO',\n",
       "  '95823',\n",
       "  'CA',\n",
       "  '3',\n",
       "  '1',\n",
       "  '1167',\n",
       "  'Residential',\n",
       "  'Wed May 21 00:00:00 EDT 2008',\n",
       "  '68212',\n",
       "  '38.478902',\n",
       "  '-121.431028']]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Strip the header from the RDD\n",
    "header = rdd.first()\n",
    "rdd = rdd.filter(lambda line:line != header)\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(baths='1', beds='2', city='SACRAMENTO', price='59222', sqft='836', street='3526 HIGH ST', zip='95838'),\n",
       " Row(baths='1', beds='3', city='SACRAMENTO', price='68212', sqft='1167', street='51 OMAHA CT', zip='95823'),\n",
       " Row(baths='1', beds='2', city='SACRAMENTO', price='68880', sqft='796', street='2796 BRANCH ST', zip='95815'),\n",
       " Row(baths='1', beds='2', city='SACRAMENTO', price='69307', sqft='852', street='2805 JANETTE WAY', zip='95815'),\n",
       " Row(baths='1', beds='2', city='SACRAMENTO', price='81900', sqft='797', street='6001 MCMAHON DR', zip='95824')]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map every line from the RDD to a Row of our soon to be DataFrame\n",
    "df = rdd.map(lambda line: Row(street = line[0], city = line[1], zip=line[2], beds=line[4], baths=line[5], sqft=line[6], price=line[9])).toDF()\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+-----+----+----------------+-----+\n",
      "|baths|beds|      city|price|sqft|          street|  zip|\n",
      "+-----+----+----------+-----+----+----------------+-----+\n",
      "|    1|   2|SACRAMENTO|59222| 836|    3526 HIGH ST|95838|\n",
      "|    1|   3|SACRAMENTO|68212|1167|     51 OMAHA CT|95823|\n",
      "|    1|   2|SACRAMENTO|68880| 796|  2796 BRANCH ST|95815|\n",
      "|    1|   2|SACRAMENTO|69307| 852|2805 JANETTE WAY|95815|\n",
      "|    1|   2|SACRAMENTO|81900| 797| 6001 MCMAHON DR|95824|\n",
      "+-----+----+----------+-----+----+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Or a little bit better visualization\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Perform a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules in order to use MLlib to do a linear regression\n",
    "import pyspark.mllib\n",
    "import pyspark.mllib.regression\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+----+\n",
      "|price|baths|beds|sqft|\n",
      "+-----+-----+----+----+\n",
      "|59222|    1|   2| 836|\n",
      "|68212|    1|   3|1167|\n",
      "|68880|    1|   2| 796|\n",
      "|69307|    1|   2| 852|\n",
      "|81900|    1|   2| 797|\n",
      "+-----+-----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the subset of features I'm interested in\n",
    "# I'm going to predict home price from the number of baths, beds, and square feet\n",
    "df = df.select('price','baths','beds','sqft')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|             baths|              beds|             price|              sqft|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|               814|               814|               814|               814|\n",
      "|   mean|1.9606879606879606|3.2444717444717446| 229448.3697788698|1591.1461916461917|\n",
      "| stddev|0.6698038253879438|0.8521372615281976|119825.57606009026| 663.8419297942894|\n",
      "|    min|                 1|                 1|            100000|              1000|\n",
      "|    max|                 5|                 8|             99000|               998|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove those rows that have suspicious 0 values for any of the features we want to use for prediction\n",
    "df = df[df.baths > 0]\n",
    "df = df[df.beds > 0]\n",
    "df = df[df.sqft > 0]\n",
    "df.describe(['baths','beds','price','sqft']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(59222.0, [1.0,2.0,836.0]),\n",
       " LabeledPoint(68212.0, [1.0,3.0,1167.0]),\n",
       " LabeledPoint(68880.0, [1.0,2.0,796.0]),\n",
       " LabeledPoint(69307.0, [1.0,2.0,852.0]),\n",
       " LabeledPoint(81900.0, [1.0,2.0,797.0])]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLlib requires that our features be expressed with LabeledPoints. \n",
    "# The required format for a labeled point is a tuple of the response value and a vector of predictors. \n",
    "# We can call 'map' on df in order to return an RDD of LabeledPoints.\n",
    "temp = df.map(lambda line:LabeledPoint(line[0],[line[1:]]))\n",
    "temp.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll be using Stochastic Gradient Descent and the scare footage of these houses is quite large \n",
    "#   in comparison to the number of bedrooms and bathrooms. \n",
    "# We'll need to scale the data first with Spark's 'StandardScaler.'\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '2', '836'),\n",
       " ('1', '3', '1167'),\n",
       " ('1', '2', '796'),\n",
       " ('1', '2', '852'),\n",
       " ('1', '2', '797')]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to scale the data, we'll have to use an RDD.\n",
    "features = df.map(lambda row: row[1:])\n",
    "features.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([1.493, 2.347, 1.2593]),\n",
       " DenseVector([1.493, 3.5206, 1.7579]),\n",
       " DenseVector([1.493, 2.347, 1.1991]),\n",
       " DenseVector([1.493, 2.347, 1.2834]),\n",
       " DenseVector([1.493, 2.347, 1.2006])]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale (Normalize) the data\n",
    "standardizer = StandardScaler()\n",
    "model = standardizer.fit(features)\n",
    "features_transform = model.transform(features)\n",
    "features_transform.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['59222',\n",
       " '68212',\n",
       " '68880',\n",
       " '69307',\n",
       " '81900',\n",
       " '89921',\n",
       " '90895',\n",
       " '91002',\n",
       " '94905',\n",
       " '98937',\n",
       " '100309',\n",
       " '106250',\n",
       " '106852',\n",
       " '107502',\n",
       " '108750']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put our labels and features back together as labeled points.\n",
    "# Because the labels (prices) are in a DataFrame and the scaled features are in the new RDD we just created.\n",
    "# I can put them together with 'zip' function, but I'll need the labels to be in an RDD first. \n",
    "# A simple mapping that grabs the zero element (price) from each row addresses this problem.\n",
    "lab = df.map(lambda row: row[0])\n",
    "lab.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('59222', DenseVector([1.493, 2.347, 1.2593])),\n",
       " ('68212', DenseVector([1.493, 3.5206, 1.7579])),\n",
       " ('68880', DenseVector([1.493, 2.347, 1.1991])),\n",
       " ('69307', DenseVector([1.493, 2.347, 1.2834])),\n",
       " ('81900', DenseVector([1.493, 2.347, 1.2006]))]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the two RDDs can be put together with 'zip' function\n",
    "transformedData = lab.zip(features_transform)\n",
    "transformedData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(59222.0, [1.49297445326,2.34703972035,1.25933593899]),\n",
       " LabeledPoint(68212.0, [1.49297445326,3.52055958053,1.7579486134]),\n",
       " LabeledPoint(68880.0, [1.49297445326,2.34703972035,1.19908063091]),\n",
       " LabeledPoint(69307.0, [1.49297445326,2.34703972035,1.28343806223]),\n",
       " LabeledPoint(81900.0, [1.49297445326,2.34703972035,1.20058701361])]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Go back to the LabeledPoint structure before using MLlib\n",
    "transformedData = transformedData.map(lambda row: LabeledPoint(row[0],[row[1]]))\n",
    "transformedData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training / testing subsets\n",
    "trainingData, testingData = transformedData.randomSplit([.8, .2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "814"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the full data size\n",
    "transformedData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "652"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the training data size\n",
    "trainingData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the testing data size\n",
    "testingData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import linear regression with stochastic gradient descent and build a model. \n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "\n",
    "# The number of iterations is specified along with the step size and the data set.\n",
    "linearModel = LinearRegressionWithSGD.train(trainingData, 1000, .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([15911.6446, 4526.9663, 68332.1903])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get coefficients and intercepts from the model.\n",
    "linearModel.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(100309.0, [2.98594890652,3.52055958053,1.36930187625]),\n",
       " LabeledPoint(124100.0, [2.98594890652,3.52055958053,2.41171870613]),\n",
       " LabeledPoint(148750.0, [2.98594890652,4.69407944071,2.21739533756]),\n",
       " LabeledPoint(150000.0, [1.49297445326,1.17351986018,1.14485085363]),\n",
       " LabeledPoint(161500.0, [2.98594890652,4.69407944071,2.3906293483]),\n",
       " LabeledPoint(166357.0, [1.49297445326,4.69407944071,2.94497818269]),\n",
       " LabeledPoint(168000.0, [2.98594890652,3.52055958053,2.22492725107]),\n",
       " LabeledPoint(178480.0, [2.98594890652,3.52055958053,1.78506350204]),\n",
       " LabeledPoint(181872.0, [1.49297445326,3.52055958053,1.73535287287]),\n",
       " LabeledPoint(182587.0, [4.47892335978,4.69407944071,2.78831438167])]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examining the first 10 rows from the testing dataset\n",
    "testingData.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158273.59605366364"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a prediction on one of the points using our model.\n",
    "linearModel.predict([1.49297445326,3.52055958053,1.73535287287])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model with metrics available within the evaluation package for MLlib\n",
    "from pyspark.mllib.evaluation import RegressionMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(120433.83152581986, 59222.0),\n",
       " (159817.6124950933, 68212.0),\n",
       " (116316.45434865377, 68880.0),\n",
       " (122080.78239668628, 69307.0),\n",
       " (116419.38877808292, 81900.0)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need an RDD that's a tuple of predictions from our model and the original home values.\n",
    "prediObserRDDin = trainingData.map(lambda row: (float(linearModel.predict(row.features[0])),row.label))\n",
    "prediObserRDDin.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calling RegressionMetrics on this RDD builds a variable that contains a number of metrics. \n",
    "metrics = RegressionMetrics(prediObserRDDin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.466721671313607"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check R squared\n",
    "metrics.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(157016.20856897274, 100309.0),\n",
       " (228246.8337339457, 124100.0),\n",
       " (220280.77716580936, 148750.0),\n",
       " (107298.33006098008, 150000.0),\n",
       " (232118.2365501618, 161500.0)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the same RDD mapping and RegressionMetrics for the test data\n",
    "prediObserRDDout = testingData.map(lambda row: (float(linearModel.predict(row.features[0])),row.label))\n",
    "metrics = RegressionMetrics(prediObserRDDout)\n",
    "prediObserRDDout.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85023.27994408192"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Root Mean Squared Error (RMSE)\n",
    "# More info about the Spark evaluation metrics can be found in the documentation for the MLlib evaluation module: \n",
    "#   https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.evaluation\n",
    "metrics.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The End#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
