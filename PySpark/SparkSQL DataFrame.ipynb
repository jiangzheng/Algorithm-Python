{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [0. Import Packages and Connect to Hive](#0)\n",
    "- [1. Use Spark SQL API](#1)\n",
    "  - [1.1. Cache / Unpersist](#1.1)\n",
    "  - [1.2. Data Schema](#1.2)\n",
    "  - [1.3. Take a Peek](#1.3)\n",
    "  - [1.4. Basic Statistics](#1.4)\n",
    "  - [1.5. Aggregation](#1.5)\n",
    "  - [1.6. Subsetting](#1.6)\n",
    "  - [1.7. Transformation](#1.7)\n",
    "  - [1.8. Sorting](#1.8)\n",
    "  - [1.9. Sampling](#1.9)\n",
    "  - [1.10. Metrics for Two Columns](#1.10)\n",
    "  - [1.11. Operations for Multiple Dataframes](#1.11)\n",
    "  - [1.12. Save to File](#1.12)\n",
    "  - [1.13. Partitions](#1.13)\n",
    "  - [1.14. Others](#1.14)\n",
    "- [2. Iterate All Tables of a Hive Database](#2)\n",
    "- [3. References](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Packages and Connect to Hive <a id='0'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import time\n",
    "from pyspark.sql import HiveContext, Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "# Use HiveContext to connect Hive\n",
    "hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create a DataFrame that contains all data from a table\n",
    "df = hc.table(\"ids_meds_mlcustomer.dbo_customeraddress\") # 4885823 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Use Spark SQL API<a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Cache / Unpersist<a id='1.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### cache() - Persists with the default storage level (MEMORY_ONLY_SER).  *** MUCH FASTER AFTER CACHE() ***\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### persist(storageLevel=StorageLevel(False, True, False, False, 1)) - Sets the storage level to persist its values \n",
    "###   across operations after the first time it is computed. This can only be used to assign a new storage level \n",
    "###   if the RDD does not have a storage level set yet. If no storage level is specified defaults to (MEMORY_ONLY_SER).\n",
    "### Reference of Storage Levels: http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence\n",
    "df.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### unpersist(blocking=True) - Marks the DataFrame as non-persistent, and remove all blocks for it from memory and disk.\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Data Schema<a id='1.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### columns - Returns all column names as a list.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### dtypes - Returns all column names and their data types as a list.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### printSchema() - Prints out the schema in the tree format.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# schema - Returns the schema of this DataFrame as a types.StructType.\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Take a Peek<a id='1.3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### count() - Returns the number of rows in this DataFrame.\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### take(num) - Returns the first num rows as a list of Row.\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### first() - Returns the first row as a Row.\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### head(n=None) - Returns the first n rows.\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### limit(num) - Limits the result count to the number specified.\n",
    "df.limit(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Select a column from the data frame\n",
    "city = df.city # or df[\"city\"]\n",
    "city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### collect() - Returns all the records as a list of Row.\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### show(n=20, truncate=True) - Prints the first n rows to the console.\n",
    "df.selectExpr(\"addresstype_id * 10\", \"concat(city, ', ', state)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Basic Statistics<a id='1.4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### describe(*cols) - Computes statistics for numeric columns.\n",
    "### Spark 1.6.1 only supports numerical columns. Spark 2.0 can also compute for String columns.\n",
    "# df = hc.sql(\"SELECT CAST(latitude AS FLOAT), CAST(longitude AS FLOAT) \\\n",
    "#                 FROM ids_weather_noaaweatherdb.noaaweatherschema_currentforecastweather\") # 91240068 rows\n",
    "print(\"Computing column-wise summary statistics ...\")\n",
    "st = time.time()\n",
    "df.describe().show()\n",
    "print(\"\\nDone (\" + str(time.time() - st) + \" sec)\") # 5M rows 26 cols - 7 sec; 91M rows 2 col - 97 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### cube(*cols) - Create a multi-dimensional cube for the current DataFrame using the specified columns for aggregation on them.\n",
    "df.cube('county', df.state).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### explain(extended=False) - Prints the (logical and physical) plans to the console for debugging purpose.\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Aggregation<a id='1.5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### groupBy(*cols) - Groups the DataFrame by the specified columns. See GroupedData for all the available aggregate functions.\n",
    "df.groupBy(['addresstype_id']).count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### freqItems(cols, support=None) - Finding frequent items for columns, possibly with false positives.\n",
    "df.freqItems(['addresstype_id'], 0.001).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# agg(*exprs) - Aggregate on the entire DataFrame without groups (shorthand for df.groupBy.agg()).\n",
    "df.agg({\"city\": \"max\"}).collect() # find the max value in alphabetical order in the \"city\" field\n",
    "# Alternatively, we can use the pyspark.sql.functions module:\n",
    "# from pyspark.sql import functions as F\n",
    "# df.agg(F.max(df.city)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### agg(*exprs) - Compute aggregates and returns the result as a DataFrame.\n",
    "###   The available aggregate functions are avg, max, min, sum, count.\n",
    "gdf = df.groupBy(df.addresstype_id)\n",
    "gdf.agg({\"*\": \"count\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### rollup(*cols) - Create a multi-dimensional rollup for the current DataFrame using the specified columns for aggregation.\n",
    "df.rollup('city', df.state).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Subsetting<a id='1.6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### filter(condition) - Filters rows using the given condition.\n",
    "df.filter(df.addresstype_id == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### select(*cols) - Projects a set of expressions and returns a new DataFrame.\n",
    "df.select('city', 'state').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### selectExpr(*expr) - Projects a set of SQL expressions and returns a new DataFrame.\n",
    "df.selectExpr(\"addresstype_id * 10\", \"concat(city, ', ', state)\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### where(condition) - Filters rows using the given condition. An alias for filter().\n",
    "df.where(df.addresstype_id == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### distinct() - Returns a new DataFrame containing the distinct rows in this DataFrame.\n",
    "print(\"Computing distinct rows ...\")\n",
    "st = time.time()\n",
    "print(df.distinct().count())\n",
    "print(\"\\nDone (\" + str(time.time() - st) + \" sec)\") # 5M rows 26 cols - 36 sec; 91M rows 2 col - ?? sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### dropDuplicates(subset=None) - Return a new DataFrame with duplicate rows removed, optionally only considering certain columns.\n",
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### dropna(how='any', thresh=None, subset=None) - Returns a new DataFrame omitting rows with null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Transformation<a id='1.7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### withColumn(colName, col) - Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "df.withColumn('citystate', concat(df.city, lit(', '), df.state)).select('city', 'state', 'citystate').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### withColumnRenamed(existing, new) - Returns a new DataFrame by renaming an existing column.\n",
    "#df.withColumnRenamed('city', 'city2').select('city2', 'state').take(5)\n",
    "df.withColumnRenamed('city2', 'city').select('city', 'state').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### drop(col) - Returns a new DataFrame that drops the specified column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### foreach(f) - Applies the f function to all Row of this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### map(f) - Returns a new RDD by applying a the f function to each Row. This is a shorthand for df.rdd.map().\n",
    "df.map(lambda p: p.city).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### flatMap(f) - Returns a new RDD by first applying the f function to each Row, and then flattening the results.\n",
    "df.flatMap(lambda p: p.city).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### replace(to_replace, value, subset=None) - Returns a new DataFrame replacing a value with another value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### na - Returns a DataFrameNaFunctions for handling missing values. (drop(), fill(), replace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### fillna(value, subset=None) - Replace null values, alias for na.fill()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### toJSON(use_unicode=True) - Converts a DataFrame into a RDD of string.\n",
    "df.toJSON().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# toDF(*cols) - Returns a new class:DataFrame that with new specified column names\n",
    "df.toDF('new_id', 'new_customer_id', 'new_addresstype_id', 'new_street1', 'new_street2', 'new_city', 'new_county', \\\n",
    "        'new_state', 'new_zipcode', 'new_latitude', 'new_longitude', 'new_unit', 'new_pobox', 'new_dateupdated', \\\n",
    "        'new_changedate', 'new_audit_id', 'new_audit_ts', 'new_audit_schema_version', 'new_audit_ds', 'new_audit_ds_db', \\\n",
    "        'new_audit_ds_db_schema', 'new_audit_ds_db_table', 'new_audit_op_type', 'new_audit_digest', 'new_audit_consumer_ts', \\\n",
    "        'new_pipeline_processed_date').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# toPandas() - Returns the contents of this DataFrame as Pandas pandas.DataFrame.\n",
    "df.toPandas()\n",
    "# For large tables, the result should be something like org.apache.spark.SparkException: \n",
    "#   Job aborted due to stage failure: Total size of serialized results of 2 tasks (1084.9 MB) \n",
    "#   is bigger than spark.driver.maxResultSize (1024.0 MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8. Sorting<a id='1.8'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### orderBy(*cols, **kwargs) - Returns a new DataFrame sorted by the specified column(s).\n",
    "df.orderBy(df.city.desc()).select(df.city).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### sort(*cols, **kwargs) - Returns a new DataFrame sorted by the specified column(s).\n",
    "df.sort(df.city.desc()).select('city', 'state').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9. Sampling<a id='1.9'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### sample(withReplacement, fraction, seed=None) - Returns a sampled subset of this DataFrame.\n",
    "df.sample(False, 0.5, 42).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### sampleBy(col, fractions, seed=None) - Returns a stratified sample without replacement based on the fraction given on each stratum.\n",
    "sampled = df.sampleBy(\"addresstype_id\", fractions={1: 0.2, 2: 0.2, 3: 0.3, 4: 0.3}, seed=0)\n",
    "sampled.groupBy(\"addresstype_id\").count().orderBy(\"addresstype_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### randomSplit(weights, seed=None) - Randomly splits this DataFrame with the provided weights.\n",
    "splits = df.randomSplit([1.0, 2.0], 24)\n",
    "print(str(splits[0].count()) + \", \" + str(splits[1].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10. Metrics for Two Columns<a id='1.10'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### corr(col1, col2, method=None) - Calculates the correlation of two columns of a DataFrame as a double value. Pearson only.\n",
    "df2 = hc.sql(\"SELECT addresstype_id, CAST(zipcode AS INT) FROM ids_meds_mlcustomer.dbo_customeraddress\") # 4885823 rows\n",
    "# df2 = hc.sql(\"SELECT CAST(latitude AS FLOAT), CAST(longitude AS FLOAT) \\\n",
    "#                 FROM ids_weather_noaaweatherdb.noaaweatherschema_currentforecastweather\") # 91240068 rows\n",
    "print(\"Computing the Pearson correlation ...\\n\")\n",
    "st = time.time()\n",
    "print(df2.corr('addresstype_id', 'zipcode', 'pearson'))\n",
    "#print(df2.corr('latitude', 'longitude', 'pearson'))\n",
    "print(\"\\nDone (\" + str(time.time() - st) + \" sec)\") # 5M rows 2 cols, 11 sec; 91M rows 2 cols, 83 sec; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### cov(col1, col2) - Calculate the sample covariance for the given columns, specified by their names, as a double value.\n",
    "#df2 = hc.sql(\"SELECT addresstype_id, CAST(zipcode AS INT) FROM ids_meds_mlcustomer.dbo_customeraddress\") # 4885823 rows\n",
    "# df2 = hc.sql(\"SELECT CAST(latitude AS FLOAT), CAST(longitude AS FLOAT) \\\n",
    "#                 FROM ids_weather_noaaweatherdb.noaaweatherschema_currentforecastweather\") # 91240068 rows\n",
    "print(\"Computing the covariance ...\\n\")\n",
    "st = time.time()\n",
    "print(df2.cov('addresstype_id', 'zipcode'))\n",
    "#print(df2.cov('latitude', 'longitude'))\n",
    "print(\"\\nDone (\" + str(time.time() - st) + \" sec)\") # 5M rows 2 cols, 6 sec; 91M rows 2 cols, 61 sec; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# crosstab(col1, col2) - Computes a pair-wise frequency table of the given columns. Also known as a contingency table.\n",
    "df.crosstab('addresstype_id', 'pobox').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11. Operations for Multiple Dataframes<a id='1.11'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### join(other, on=None, how=None) - Joins with another DataFrame, using the given join expression.\n",
    "df3 = hc.table(\"ids_meds_mlcustomer.dbo_customer\") # 3168313 rows\n",
    "df.join(df3, df.customer_id == df3.client_id, 'outer').select(df.customer_id, df.addresstype_id, df3.lastname).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### intersect(other) - Return a new DataFrame containing rows only in both this frame and another frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### unionAll(other) - Return a new DataFrame containing union of rows in this frame and another frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### subtract(other) - Return a new DataFrame containing rows in this frame but not in another frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.12. Save to File<a id='1.12'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### write - Interface for saving the content of the DataFrame out into external storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.13. Partitions<a id='1.13'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coalesce(numPartitions) - Returns a new DataFrame that has exactly numPartitions partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# foreachPartition(f) - Applies the f function to each partition of this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mapPartitions(f, preservesPartitioning=False) - Returns a new RDD by applying the f function to each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# repartition(numPartitions, *cols) - Returns a new DataFrame partitioned by the given partitioning expressions.\n",
    "#   The resulting DataFrame is hash partitioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sortWithinPartitions(*cols, **kwargs) - Returns a new DataFrame with each partition sorted by the specified column(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.14. Others<a id='1.14'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alias(alias) - Returns a new DataFrame with an alias set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop_duplicates(subset=None) - An alias for dropDuplicates()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# groupby(*cols) - An alias for groupBy()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# isLocal() - Returns True if the collect() and take() methods can be run locally (without any Spark executors).\n",
    "df.isLocal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rdd - Returns the content as an pyspark.RDD of Row.\n",
    "df.rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# registerTempTable(name) - Registers this RDD as a temporary table using the given name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stat - Returns a DataFrameStatFunctions for statistic functions. (Covered by other APIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Iterate All Tables of a Hive Database<a id='2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify a database\n",
    "database_name = \"ids_meds_mlcustomer\"\n",
    "# database_name = \"ids_meds_mlinterview\"\n",
    "# database_name = \"ids_meds_mlpolicy\"\n",
    "# database_name = \"ids_meds_mlreference\"\n",
    "\n",
    "tables = hc.tableNames(database_name) # Get all table names in a database\n",
    "#tables = [\"dbo_customeraddress\"] # Get specific tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Iterate tables in the specified database (ignore this cell for now)\n",
    "start = time.time()\n",
    "for n in tables:\n",
    "    table_name = database_name + \".\" + n\n",
    "    print(\"Profiling \" + table_name + \" ...\"),\n",
    "    st = time.time()\n",
    "    \n",
    "    #rdd_row = hc.table(\"ids_meds_mlcustomer.dbo_addresstype\") # Get all columns for profiling\n",
    "\n",
    "    print(\"Done (\" + str(time.time() - st) + \" sec)\")\n",
    "print(\"Total Time: \" + str(time.time() - start) + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The End #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
